{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization project: clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "import functools\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum Spanning Tree Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing all the distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "def compute_dist(df):\n",
    "    result = []\n",
    "    for i in range(len(df)):\n",
    "        for j in range(i+1,len(df)):\n",
    "            # We use the package \"distance\" to compute the euclidean distance between\n",
    "            # the different points in the dataset\n",
    "            d = distance.euclidean(df.iloc[i].to_list(),df.iloc[j].to_list())\n",
    "            result.append([i, j, d])\n",
    "\n",
    "    ordered_result = sorted(result, key=lambda t: t[::-1])\n",
    "    return ordered_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the minimum spanning tree it makes sense to differentiate the following cases:\n",
    "\n",
    "    case 1: None of them are in a cluster:\n",
    "    case 2: Only one of them is already in a cluster:\n",
    "    case 3: Both of them are already in a cluster:\n",
    "        --> 3a: Both of them are in the same cluster: do nothing\n",
    "        --> 3b: They are in different clusters: merge the 2 corresponding clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MST_clustering(K, N, ordered_result):\n",
    "    # We initialize the variables that we will need in the outter for loop\n",
    "    cluster_dict = {}\n",
    "    k = 0\n",
    "    vertex = []\n",
    "    counter = 0\n",
    "    i = 0\n",
    "    final_number_clusters = N-K\n",
    "    for ite in ordered_result:\n",
    "        \n",
    "        # Here we set the condition to include as clusters the points that are left alone in the algorithm. \n",
    "        # At this point we will iterate through the rest of edges and we will add the points which have not been\n",
    "        # visited before (if they have been visited, they already are in one component of the MST)\n",
    "        \n",
    "        if counter == final_number_clusters:\n",
    "            for ite in ordered_result[i:]:\n",
    "                if ite[0] not in vertex:\n",
    "                    k += 1\n",
    "                    cluster_dict[k] = [ite[0]]\n",
    "                    vertex.append(ite[0])\n",
    "                if ite[1] not in vertex:\n",
    "                    k += 1\n",
    "                    cluster_dict[k] = [ite[1]]  \n",
    "                    vertex.append(ite[1])\n",
    "            return cluster_dict\n",
    "    \n",
    "        # Here we want to know whether the vertices of the edge of this iteartion have already been included in any\n",
    "        # of the components of the MST. We assign a key to these two vertices. We assign key = -1 to the vertices \n",
    "        # that have not been visited before, and if the vertex have already been assigned to a cluster then we assign\n",
    "        # to him its component/cluster, so key = cluster.\n",
    "        \n",
    "        key_0 = -1\n",
    "        key_1 = -1\n",
    "        \n",
    "        if ite[0] not in vertex:\n",
    "            pass\n",
    "        else:\n",
    "            for cluster in cluster_dict:\n",
    "                if ite[0] in cluster_dict[cluster]:\n",
    "                    key_0 = cluster\n",
    "                    \n",
    "        if ite[1] not in vertex:\n",
    "            pass\n",
    "        else:\n",
    "            for cluster in cluster_dict:\n",
    "                if ite[1] in cluster_dict[cluster]:\n",
    "                    key_1 = cluster\n",
    "\n",
    "                    \n",
    "        # Now, we have four different cases. (a) None of the vertices have been added to a cluster, (b) one of the\n",
    "        # vertices has been added to a cluster, (c) both of them have been added to the same cluster (so we will not\n",
    "        # do anything given that we would be creating a cycle) and (d) both of them have been added to a cluster, but\n",
    "        # each of them is in a different cluster.\n",
    "        \n",
    "        # case (a)\n",
    "        \n",
    "        if (key_0 == -1) and (key_1 == -1):\n",
    "            k += 1\n",
    "            cluster_dict[k] = [ite[0]] \n",
    "            cluster_dict[k] += [ite[1]]\n",
    "            counter +=1\n",
    "            vertex.append(ite[0])\n",
    "            vertex.append(ite[1])\n",
    "            \n",
    "        # case (b)\n",
    "        \n",
    "        elif (key_0 == -1) and (key_1 != -1):\n",
    "            cluster_dict[key_1] += [ite[0]]\n",
    "            counter +=1\n",
    "            vertex.append(ite[0])\n",
    "        elif (key_0 != -1) and (key_1 == -1):\n",
    "            cluster_dict[key_0] += [ite[1]]\n",
    "            counter +=1\n",
    "            vertex.append(ite[1])\n",
    "            \n",
    "        # case (c) and (d)\n",
    "        else:\n",
    "            \n",
    "            # case (c)\n",
    "            if key_0 == key_1:\n",
    "                pass\n",
    "            \n",
    "            # case (d)\n",
    "            else:\n",
    "                cluster_dict[key_0] += cluster_dict[key_1]\n",
    "                del cluster_dict[key_1]\n",
    "                counter +=1\n",
    "        i +=1\n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_2(cluster_dict):\n",
    "    ind_list = []\n",
    "    clust_list = []\n",
    "\n",
    "    for k, v in cluster_dict.items():\n",
    "        [clust_list.append(k) for i in v]\n",
    "        [ind_list.append(val) for val in v]\n",
    "\n",
    "    df_s = pd.DataFrame()\n",
    "    df_s['index'] = ind_list\n",
    "    df_s['Cluster'] = clust_list\n",
    "    \n",
    "    df_s = df_s.sort_values('index').set_index('index')\n",
    "    return df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_5(cluster_dict):\n",
    "    ind_list = []\n",
    "    clust_list = []\n",
    "\n",
    "    for k, v in cluster_dict.items():\n",
    "        [clust_list.append(k) for i in v]\n",
    "        [ind_list.append(val) for val in v]\n",
    "\n",
    "    df_s = pd.DataFrame()\n",
    "    df_s['index'] = ind_list\n",
    "    df_s['Cluster'] = clust_list\n",
    "    \n",
    "    df_s = df_s.sort_values('index').set_index('index')\n",
    "    return df_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Davies Bouldin algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Davies_Bouldin(cluster_dict,df):\n",
    "    clusters = []\n",
    "    for n in cluster_dict:\n",
    "        clusters.append(n)\n",
    "    centroid_dict = {}\n",
    "    for n in clusters:\n",
    "        mask = (df.Cluster == n)\n",
    "        nice = df[mask]\n",
    "        centroid = nice.mean()\n",
    "        centroid = centroid.to_list()\n",
    "        list_ = []\n",
    "        for el in range(0,len(list(df.columns[:-1]))):\n",
    "            list_.append(centroid[el])\n",
    "            centroid_dict[n] = list_\n",
    "    S_i = {}\n",
    "    for n in cluster_dict:\n",
    "        sum_ = 0\n",
    "        for el in cluster_dict[n]:\n",
    "            sum_ += distance.euclidean(df.iloc[el][:-1],centroid_dict[n])\n",
    "            average = sum_/len(cluster_dict[n])\n",
    "            S_i[n] = average\n",
    "    M_ij = {}\n",
    "    centroids = list(centroid_dict.keys())\n",
    "    for i in range(0,len(centroids)):\n",
    "        for j in range(i+1,len(centroids)):\n",
    "            d = distance.euclidean(centroid_dict[centroids[i]],centroid_dict[centroids[j]])\n",
    "            M_ij[(centroids[i],centroids[j])] = d\n",
    "    dispersion = list(S_i.keys())\n",
    "    D_i = {}\n",
    "    for i in range(0, len(dispersion)):\n",
    "        D_i[dispersion[i]] = 0\n",
    "        for j in range(0,len(dispersion)):\n",
    "            if i!=j:\n",
    "                try:\n",
    "                    R_ij = (S_i[dispersion[i]]+S_i[dispersion[j]])/(M_ij[(dispersion[i],dispersion[j])])\n",
    "                    if R_ij >= D_i[dispersion[i]]:\n",
    "                        D_i[dispersion[i]] = R_ij\n",
    "                except:\n",
    "                    R_ij = (S_i[dispersion[i]]+S_i[dispersion[j]])/(M_ij[(dispersion[j],dispersion[i])])\n",
    "                    if R_ij >= D_i[dispersion[i]]:\n",
    "                        D_i[dispersion[i]] = R_ij\n",
    "    count = 0\n",
    "    for n in D_i:\n",
    "        count += D_i[n]\n",
    "    DB = count/len(D_i)\n",
    "    return DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dunn Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_within(ordered_result, cluster_dict):\n",
    "    for i in range(len(ordered_result)):\n",
    "        for k,v in cluster_dict.items():\n",
    "            if ordered_result[-i-1][0] in v and ordered_result[-i-1][1] in v:\n",
    "                return ordered_result[-i-1][2]\n",
    "\n",
    "def min_between(ordered_result, cluster_dict):\n",
    "    for i in range(len(ordered_result)):\n",
    "        for k,v in cluster_dict.items():\n",
    "            if ordered_result[i][0] in v and ordered_result[i][1] not in v:\n",
    "                return ordered_result[i][2]\n",
    "\n",
    "def dunn(ordered_result, cluster_dict):\n",
    "    num = max_within(ordered_result, cluster_dict)\n",
    "    den = min_between(ordered_result, cluster_dict)\n",
    "    return num/den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the dataset with 2 columns ('Synthetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('synthetic_clean.csv')\n",
    "N = len(df)\n",
    "ordered_result = compute_dist(df)\n",
    "\n",
    "def main_MST(k, df):\n",
    "\n",
    "    cluster_dict = MST_clustering(k, N, ordered_result)\n",
    "    \n",
    "    df['Cluster']= get_df_2(cluster_dict)['Cluster']\n",
    "    \n",
    "    DB = Davies_Bouldin(cluster_dict,df)\n",
    "\n",
    "    Dunn = dunn(ordered_result, cluster_dict)\n",
    "    \n",
    "    return DB, Dunn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB, Dunn = main_MST(15,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the dataset with 5 columns ('Thyroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('thyroid_clean.csv')\n",
    "N = len(df)\n",
    "ordered_result = compute_dist(df)\n",
    "\n",
    "def main(K, df):\n",
    "\n",
    "    cluster_dict = MST_clustering(K, N, ordered_result)\n",
    "    \n",
    "    df['Cluster']= get_df_5(cluster_dict)['Cluster']\n",
    "    \n",
    "    DB_MST = Davies_Bouldin(cluster_dict,df)\n",
    "\n",
    "    Dunn_MST = dunn(ordered_result, cluster_dict)\n",
    "    \n",
    "    package_MST = davies_bouldin_score(df.drop('Cluster',axis=1), labels = df['Cluster'].to_list())\n",
    "    \n",
    "    return DB_MST, Dunn_MST, package_MST\n",
    "\n",
    "n_clusters = []\n",
    "DB_MST = []\n",
    "Dunn_MST = []\n",
    "Package_MST = []\n",
    "for K in range(2, N-1, 10):\n",
    "    n_clusters.append(K)\n",
    "    DB_MST.append(main(K, df)[0])\n",
    "    Dunn_MST.append(main(K,df)[1])\n",
    "    Package_MST.append(main(K,df)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot(n_clusters,DB_MST)\n",
    "plt.show()\n",
    "sns.lineplot(n_clusters,Package_MST)\n",
    "plt.show()\n",
    "sns.lineplot(n_clusters,Dunn_MST)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly assign indeces of cluster centroids:\n",
    "def initiate_centroids(n, df):\n",
    "    centroids = []\n",
    "    random.seed(42) # use this since we want to compare the different number of clusters\n",
    "    \n",
    "    #generate random centroid indeces\n",
    "    initial_index_centroid = random.sample(range(0, len(df)), n)\n",
    "    \n",
    "    #find the data points corresponding the the indeces:\n",
    "    for i in initial_index_centroid:\n",
    "        centroids.append(df.loc[i])\n",
    "    return centroids\n",
    "\n",
    "\n",
    "# To calculate the distance between two points:\n",
    "def calc_distance(X1, X2):\n",
    "    return(sum((X1 - X2)**2))**0.5\n",
    "\n",
    "\n",
    "# To find the closest centroid to each data point:\n",
    "def findClosestCentroids(centroids, df):\n",
    "    assigned_centroid = []\n",
    "    \n",
    "    #iterate over every data point in the dataframe:\n",
    "    for index, row in df.iterrows():\n",
    "        distance=[]\n",
    "        \n",
    "        #find distance of data point with each cluster:\n",
    "        for center in centroids:\n",
    "            distance.append(calc_distance(row[:-1], center[:-1]))\n",
    "            \n",
    "        #assign data point to closest cluster:\n",
    "        assigned_centroid.append(np.argmin(distance))\n",
    "    return assigned_centroid\n",
    "\n",
    "\n",
    "#To update the centroid of the clusters:\n",
    "def calc_centroids(clusters, df):\n",
    "    \n",
    "    #initiate empty list for new centroids of each cluster:\n",
    "    new_centroids = []\n",
    "    \n",
    "    #df including each point and its respective cluster\n",
    "    new_df = pd.concat([pd.DataFrame(df), pd.DataFrame(clusters, columns=['cluster'])],\n",
    "                      axis=1)\n",
    "    \n",
    "    #iterate over the distinct clusters\n",
    "    for c in set(new_df['cluster']):\n",
    "        \n",
    "        #take out the data points corresponding to each cluster:\n",
    "        current_cluster = new_df[new_df['cluster'] == c][new_df.columns[:-1]]\n",
    "        \n",
    "        #find the new cluster centroid which is the mean of the clusters we already assigned\n",
    "        cluster_mean = current_cluster.mean(axis=0)\n",
    "        \n",
    "        #append the new centroid\n",
    "        new_centroids.append(cluster_mean)\n",
    "        \n",
    "    return new_centroids\n",
    "\n",
    "\n",
    "#Recursively find and update cluster centroids:\n",
    "#n: number of clusters, df: dataframe of data points, iterations: number of iterations\n",
    "def recursive_centroid_find(n, df, iterations):\n",
    "    #initiate centroids:\n",
    "    centroids = initiate_centroids(n, df)\n",
    "    # Recursively call the functions again to update the mean of the clusters:\n",
    "    for i in range(iterations):\n",
    "        get_centroids = findClosestCentroids(centroids, df)\n",
    "        centroids = calc_centroids(get_centroids, df)\n",
    "        #print(pd.DataFrame(centroids))\n",
    "        \n",
    "        #plot the centroids after every iteration:\n",
    "        #plt.figure()\n",
    "        #plt.scatter(np.array(centroids)[:, 0], np.array(centroids)[:, 1], color='red')\n",
    "        #plt.scatter(df.X1, df.X2, alpha=0.1)\n",
    "        #plt.show()\n",
    "    return pd.DataFrame(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_dict_2(df):\n",
    "    cluster_dict = {}\n",
    "    visited = []\n",
    "    for n in range(0,len(df)):\n",
    "        if int(df.iloc[n][2]) in visited:\n",
    "            cluster_dict[int(df.iloc[n][2])] += [n]\n",
    "        else:\n",
    "            cluster_dict[int(df.iloc[n][2])] = [n]\n",
    "            visited.append(int(df.iloc[n][2]))\n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_dict_5(df):\n",
    "    cluster_dict = {}\n",
    "    visited = []\n",
    "    for n in range(0,len(df)):\n",
    "        if int(df.iloc[n][5]) in visited:\n",
    "            cluster_dict[int(df.iloc[n][5])] += [n]\n",
    "        else:\n",
    "            cluster_dict[int(df.iloc[n][5])] = [n]\n",
    "            visited.append(int(df.iloc[n][5]))\n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the dataset with 2 columns ('Synthetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "X1, X2 = simulate_data_2(N)\n",
    "df = pd.DataFrame({'X1': X1, 'X2': X2}, columns=['X1', 'X2'])\n",
    "\n",
    "def main_KMeans(k, df):\n",
    "    \n",
    "    centroids = initiate_centroids(k, df)\n",
    "    df['Cluster'] = findClosestCentroids(centroids, df)\n",
    "    cluster_dict = get_cluster_dict_2(df)\n",
    "    DB_KMeans = Davies_Bouldin(cluster_dict,df)\n",
    "    Dunn = dunn(ordered_result, cluster_dict)\n",
    "    \n",
    "    return DB_KMeans, Dunn_KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_KMeans, Dunn_KMeans = main_KMeans(15,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dunn_KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the dataset with 5 columns ('Thyroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('thyroid_clean.csv')\n",
    "N = len(df)\n",
    "ordered_result = compute_dist(df)\n",
    "\n",
    "def main_KMeans(k, df):\n",
    "    \n",
    "    centroids = initiate_centroids(k, df)\n",
    "    df['Cluster'] = findClosestCentroids(centroids, df)\n",
    "    cluster_dict = get_cluster_dict_5(df)\n",
    "    DB_KMeans = Davies_Bouldin(cluster_dict,df)\n",
    "    Dunn_KMeans = dunn(ordered_result, cluster_dict)\n",
    "    \n",
    "    return DB_KMeans, Dunn_KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = []\n",
    "DB_KMeans = []\n",
    "Dunn_KMeans = []\n",
    "for K in range(2, N-1, 10):\n",
    "    n_clusters.append(K)\n",
    "    DB_KMeans.append(main_KMeans(K, df)[0])\n",
    "    Dunn_KMeans.append(main_KMeans(K,df)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot(n_clusters,DB_KMeans)\n",
    "plt.show()\n",
    "sns.lineplot(n_clusters,Dunn_KMeans)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
