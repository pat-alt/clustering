{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tic = time.time()\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "import functools\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum spanning tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Kruskal's algorithm to find the minimum spanning tree we need to first compute distances of all $\\frac{N(N-1)}{2}$ possible edges that connect the $n$ vertices. To compute the distances we rely on the Euclidean distance\n",
    "\n",
    "$$\n",
    "d(x,y) = \\sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + ... + (x_N-y_N)^2}\n",
    "$$\n",
    "\n",
    "which in Python can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist(df):\n",
    "    size = len(df)\n",
    "    result = []\n",
    "    arr = df.values\n",
    "    for i in range(size):\n",
    "        for j in range(i+1,size):\n",
    "            # We use the package \"distance\" to compute the euclidean distance between\n",
    "            # the different points in the dataset\n",
    "            result.append([i, j, math.sqrt(sum((arr[i] - arr[j])**2))])\n",
    "\n",
    "    ordered_result = sorted(result, key=lambda t: t[::-1])\n",
    "    return ordered_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function also stores the corresponding edges and sorts them in terms of non-decreasing order of distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose the following function to compute clusters through MST clustering. It takes as inputs the number of clusters $K$, the number of vertices $N$ and the ordered list of edges we just computed above. The code is commmented, but let us lay out the high-level methodology upfront. To find the minimum spanning tree it makes sense to differentiate the following cases:\n",
    "\n",
    "- case 1: None of them are in a cluster:\n",
    "- case 2: Only one of them is already in a cluster:\n",
    "- case 3: Both of them are already in a cluster:\n",
    "    - 3a: Both of them are in the same cluster: do nothing\n",
    "    - 3b: They are in different clusters: merge the 2 corresponding clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MST_Clustering(K, N, ordered_result):\n",
    "    counter = 0\n",
    "    cluster_dict = {-1:[]}\n",
    "    vertex = []\n",
    "    Cluster_Matrix= pd.DataFrame({\"vertex\":range(N), \"cluster\":-1})\n",
    "    final_number_clusters = N-K\n",
    "    for ite in ordered_result:\n",
    "        key_0 = Cluster_Matrix.loc[ite[0], 'cluster']\n",
    "        key_1 = Cluster_Matrix.loc[ite[1], 'cluster']\n",
    "        if (key_0 == key_1) and (key_0 != -1):\n",
    "            pass\n",
    "        else:\n",
    "            counter += 1\n",
    "            if (key_0 == -1) and (key_1 == -1):\n",
    "                Cluster_Matrix.loc[ite[0], 'cluster'] = max(Cluster_Matrix.cluster) +1\n",
    "                Cluster_Matrix.loc[ite[1], 'cluster'] = max(Cluster_Matrix.cluster)\n",
    "                cluster_dict[max(cluster_dict) +1] = [ite[0]]\n",
    "                cluster_dict[max(cluster_dict)] += [ite[1]]\n",
    "                vertex.append(ite[0])\n",
    "                vertex.append(ite[1])\n",
    "            elif (key_0 == -1) or (key_1 == -1):\n",
    "                if key_0 == -1:\n",
    "                    Cluster_Matrix.loc[ite[0], 'cluster'] = key_1\n",
    "                    cluster_dict[key_1] += [ite[0]]\n",
    "                    vertex.append(ite[0])\n",
    "                else:\n",
    "                    Cluster_Matrix.loc[ite[1], 'cluster'] = key_0\n",
    "                    cluster_dict[key_0] += [ite[1]]\n",
    "                    vertex.append(ite[1])\n",
    "            else:\n",
    "                Cluster_Matrix.loc[Cluster_Matrix['cluster'] == key_1, 'cluster'] = key_0\n",
    "                cluster_dict[key_0] += cluster_dict[key_1]\n",
    "                del cluster_dict[key_1]\n",
    "        if counter == final_number_clusters:\n",
    "            for i in range(N):\n",
    "                if Cluster_Matrix.loc[i, 'cluster'] == -1:\n",
    "                    Cluster_Matrix.loc[i, 'cluster'] = max(Cluster_Matrix.cluster) +1\n",
    "                    cluster_dict[max(cluster_dict) +1] = [i]\n",
    "                    vertex.append(i)\n",
    "            del cluster_dict[-1]\n",
    "            return cluster_dict, Cluster_Matrix.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a helpfer function serialize the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(cluster_dict):\n",
    "    ind_list = []\n",
    "    clust_list = []\n",
    "\n",
    "    for k, v in cluster_dict.items():\n",
    "        [clust_list.append(k) for i in v]\n",
    "        [ind_list.append(val) for val in v]\n",
    "\n",
    "    df_s = pd.DataFrame()\n",
    "    df_s['index'] = ind_list\n",
    "    df_s['Cluster'] = clust_list\n",
    "    \n",
    "    df_s = df_s.sort_values('index').set_index('index')\n",
    "    return df_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of K-means in Python can be found below. Let us briefly outline the high-level details of the algorithm here. The basic idea is as follows:\n",
    "\n",
    "1. Randomly draw K observations from the data to be used as the intial centroids $C^{(0)}_{k}$ of the K clusters.\n",
    "2. Compute some meaure of distance (we choose Euclidean norm) between data point $x_{i,-K}$ and all centroids $C_{k}^{(0)}$ and assign $x_{i,-K}$ to its nearest centroid. Procede like this for all $i$. \n",
    "3. Compute new centroids $C_{k}^{(1)}$ as the means of all points allocated to cluster $k$. \n",
    "\n",
    "Repeat steps (2) and (3) until convergence: $C_{k}^{(t-1)}-C_{k}^{(t)}=0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly assign indeces of cluster centroids:\n",
    "def initiate_centroids(k, df):\n",
    "    centroids = []\n",
    "    random.seed(10) #for reproducability  \n",
    "    #generate random centroid indeces\n",
    "    initial_index_centroid = random.sample(range(0, len(df)), k)\n",
    "    #find the data points corresponding the the indeces:\n",
    "    for i in initial_index_centroid:\n",
    "        centroids.append(df.loc[i])\n",
    "    return np.array(centroids)\n",
    "\n",
    "# To find the closest centroid to each data point:\n",
    "def findClosestCentroids(centroids, df):\n",
    "    assigned_centroid = []\n",
    "    arr = df.to_numpy()\n",
    "    #iterate over every data point in the dataframe:\n",
    "    for row in arr:\n",
    "        distance=[]\n",
    "        #find distance of data point with each cluster:\n",
    "        for center in centroids:          \n",
    "            distance.append(np.linalg.norm(row-center))     \n",
    "        #assign data point to closest cluster:\n",
    "        assigned_centroid.append(np.argmin(distance))\n",
    "    return assigned_centroid\n",
    "\n",
    "\n",
    "#To update the centroid of the clusters:\n",
    "def calc_centroids(clusters, df): \n",
    "    #initiate empty list for new centroids of each cluster:\n",
    "    new_centroids = []\n",
    "    #df including each point and its respective cluster \n",
    "    arr = np.c_[df.to_numpy(), clusters]\n",
    "    #iterate over the distinct clusters\n",
    "    for c in np.unique(clusters):    \n",
    "        #take out the data points corresponding to each cluster:\n",
    "        current_arr = arr[arr[:, -1] == c][:,:-1]   \n",
    "        #find the new cluster centroid which is the mean of the clusters we already assigned\n",
    "        cluster_mean = current_arr.mean(axis=0)                                          \n",
    "        #append the new centroid\n",
    "        new_centroids.append(cluster_mean)   \n",
    "    return np.vstack(new_centroids)\n",
    "\n",
    "#Recursively find and update cluster centroids:\n",
    "#n: number of clusters, df: dataframe of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeans_Clustering(k, df):\n",
    "    #initiate centroids:\n",
    "    old_centroids = initiate_centroids(k, df)\n",
    "    #initiate new centroids (do this for first while loop condition to work)\n",
    "    new_centroids = calc_centroids(findClosestCentroids(old_centroids, df), df)\n",
    "    # Iterate until the absoulte difference between the coordinates of all centroids \n",
    "    #does not change, ie: the k-means algorithm converges:\n",
    "    centroids = [old_centroids, new_centroids]\n",
    "    i = 0\n",
    "    while np.any(np.absolute(np.array(new_centroids) - np.array(old_centroids)) > 0):\n",
    "        #set new centroids as the old ones:\n",
    "        i += 1\n",
    "        old_centroids = new_centroids\n",
    "        #find the new ones:\n",
    "        new_centroids = calc_centroids(findClosestCentroids(old_centroids, df), df)\n",
    "        centroids += [new_centroids]\n",
    "        #print(np.array(new_centroids))\n",
    "    return findClosestCentroids(new_centroids, df), centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a small helper function to turn the K-means output into a more convenient format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroids_to_df(cent):\n",
    "    df = pd.DataFrame(cent[0])\n",
    "    k = np.shape(cent)[1]\n",
    "    df['k'] = range(k)\n",
    "    df['iter'] = 0\n",
    "    for i in range(1,np.shape(cent)[0]):\n",
    "        df_new = pd.DataFrame(cent[i])\n",
    "        df_new['k'] = range(k)\n",
    "        df_new['iter'] = i \n",
    "        df = pd.concat([df, df_new], axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And another small helper to serialize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary(df):\n",
    "    my_dict = df.iloc[:,0].to_dict()\n",
    "    list_of_tup = [(v,k) for k,v in my_dict.items()]\n",
    "    my_dict = {}\n",
    "    for i in range(len(list_of_tup)):\n",
    "        k = list_of_tup[i][0]\n",
    "        v = list_of_tup[i][1]\n",
    "        if k not in my_dict.keys():\n",
    "            my_dict[k] = [v]\n",
    "        else:\n",
    "            my_dict[k] += [v]\n",
    "    my_dict\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the benchmark we first code up the two indices to assess clustering output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davies Bouldin algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Davies Bouldin algorithm defined as \n",
    "\n",
    "$$\n",
    "DB = \\frac{1}{K} \\sum_{i=1}^K D_i\n",
    "$$\n",
    "\n",
    "can be coded up in Python as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Davies_Bouldin(cluster_dict, df):\n",
    "    \n",
    "    # We compute the index of the clusters \n",
    "    \n",
    "    clusters = []\n",
    "    for n in cluster_dict:\n",
    "        clusters.append(n)\n",
    "        \n",
    "    # We compute the coordinates of the centroid for each cluster\n",
    "    \n",
    "    centroid_dict = {}\n",
    "    for n in clusters:\n",
    "        mask = (df.Cluster == n)\n",
    "        cluster_df = df[mask]\n",
    "        centroid = cluster_df.mean()\n",
    "        centroid = centroid.to_list()\n",
    "        list_ = []\n",
    "        for el in range(0,len(list(df.columns[:-1]))):\n",
    "            list_.append(centroid[el])\n",
    "            centroid_dict[n] = list_\n",
    "            \n",
    "    # We compute the measure of the scatter within the cluster, S_i\n",
    "\n",
    "    S_i = {}\n",
    "    for n in cluster_dict:\n",
    "        sum_ = 0\n",
    "        for el in cluster_dict[n]:\n",
    "            sum_ += distance.euclidean(df.iloc[el][:-1],centroid_dict[n])\n",
    "            average = sum_/len(cluster_dict[n])\n",
    "            S_i[n] = average\n",
    "            \n",
    "    # Compute the distance between the clusters, i.e., the distance between the centroids of the clusters\n",
    "    \n",
    "    M_ij = {}\n",
    "    centroids = list(centroid_dict.keys())\n",
    "    for i in range(0,len(centroids)):\n",
    "        for j in range(i+1,len(centroids)):\n",
    "            d = distance.euclidean(centroid_dict[centroids[i]],centroid_dict[centroids[j]])\n",
    "            M_ij[(centroids[i],centroids[j])] = d\n",
    "    \n",
    "    # We compute D_i by computig in a forloop R_ij\n",
    "    \n",
    "    dispersion = list(S_i.keys())\n",
    "    D_i = {}\n",
    "    for i in range(0, len(dispersion)):\n",
    "        D_i[dispersion[i]] = 0\n",
    "        for j in range(0,len(dispersion)):\n",
    "            if i!=j:\n",
    "                try:\n",
    "                    R_ij = (S_i[dispersion[i]]+S_i[dispersion[j]])/(M_ij[(dispersion[i],dispersion[j])])\n",
    "                    if R_ij >= D_i[dispersion[i]]:\n",
    "                        D_i[dispersion[i]] = R_ij\n",
    "                except:\n",
    "                    R_ij = (S_i[dispersion[i]]+S_i[dispersion[j]])/(M_ij[(dispersion[j],dispersion[i])])\n",
    "                    if R_ij >= D_i[dispersion[i]]:\n",
    "                        D_i[dispersion[i]] = R_ij\n",
    "                        \n",
    "    count = 0\n",
    "    for n in D_i:\n",
    "        count += D_i[n]\n",
    "    DB = count/len(D_i)\n",
    "    return DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dunn Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dunn index defined as \n",
    "\n",
    "$$\n",
    "DU = \\frac{\\min_{i\\ne j} \\delta (C_i,C_j)}{\\max_i \\Delta_i}\n",
    "$$\n",
    "\n",
    "which can be coded up in Python as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_within(ordered_result, cluster_dict):\n",
    "    for i in range(len(ordered_result)):\n",
    "        for k,v in cluster_dict.items():\n",
    "            if ordered_result[-i-1][0] in v and ordered_result[-i-1][1] in v:\n",
    "                return ordered_result[-i-1][2]\n",
    "\n",
    "def min_between(ordered_result, cluster_dict):\n",
    "    for i in range(len(ordered_result)):\n",
    "        for k,v in cluster_dict.items():\n",
    "            if ordered_result[i][0] in v and ordered_result[i][1] not in v:\n",
    "                return ordered_result[i][2]\n",
    "\n",
    "def dunn(ordered_result, cluster_dict):\n",
    "    num = max_within(ordered_result, cluster_dict)\n",
    "    den = min_between(ordered_result, cluster_dict)\n",
    "    return num/den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We summarize the benchmark process in a small wrapper function which takes as inputs the number of clusters $K$ and the data matrix. It returns the two benchmark indices introduced above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(K, df, method):\n",
    "    if method == \"kmeans\":\n",
    "        labels, cent = KMeans_Clustering(K, df)\n",
    "        df = pd.concat([pd.Series(labels), df], axis=1)\n",
    "        df.columns = [\"Cluster\"] +  list(df.columns)[1:]\n",
    "        cluster_dict = get_dictionary(df)\n",
    "    else:\n",
    "        cluster_dict, col_clust = MST_Clustering(K, N, ordered_result)\n",
    "        df['Cluster']= get_df(cluster_dict)['Cluster']\n",
    "    \n",
    "    DB_MST = Davies_Bouldin(cluster_dict,df)\n",
    "    Dunn_MST = dunn(ordered_result, cluster_dict)\n",
    "   \n",
    "    return DB_MST, Dunn_MST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thyroid data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thyroid data is small $N$ and (relatively) higher $p$ (dimension). Here we will use the wrapper function intorduced above to investigate how the custering performance varies as we change the number of clusters $K$. The provided text data was preprocessed and saved as `.csv` files. We first import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thyroid = pd.read_csv('thyroid_clean.csv')\n",
    "N = len(df_thyroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute the distances of all possible edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 588 ms, sys: 23.4 ms, total: 611 ms\n",
      "Wall time: 868 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ordered_result = compute_dist(df_thyroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for $K$ ranging from 2 to $N$ at regular intervals of 10 we perform the two clustering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.) MST clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_clusters = []\n",
    "DB_MST = []\n",
    "Dunn_MST = []\n",
    "for K in range(2, N+1, 10):\n",
    "    n_clusters.append(K)\n",
    "    DB, Dunn = benchmark(K, df_thyroid, method=\"mst\")\n",
    "    DB_MST.append(DB)\n",
    "    Dunn_MST.append(Dunn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot(n_clusters,DB_MST)\n",
    "plt.show()\n",
    "sns.lineplot(n_clusters,Dunn_MST)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.) K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_clusters = []\n",
    "DB_km = []\n",
    "Dunn_km = []\n",
    "for K in range(2, N+1, 10):\n",
    "    n_clusters.append(K)\n",
    "    DB, Dunn = benchmark(K, df_thyroid, method=\"kmeans\")\n",
    "    DB_km.append(DB)\n",
    "    Dunn_km.append(Dunn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot(n_clusters,DB_km)\n",
    "plt.show()\n",
    "sns.lineplot(n_clusters,Dunn_km)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synthetic is higher $N$ and 2-dimensional, so clusters can be easily visualized. Here we are therefore interested in seeing how well the two methods group observations into $K=15$ clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('synthetic_clean.csv')\n",
    "N = len(df)\n",
    "k = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we run the two different clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_result = compute_dist(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.) MST clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cluster_dict, cluster_col  = MST_Clustering(k, len(df), ordered_result)\n",
    "df_labelled = get_df(cluster_dict)\n",
    "df_labelled = pd.concat([df_labelled, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i df_labelled\n",
    "library(ggplot2)\n",
    "library(gganimate)\n",
    "library(data.table)\n",
    "library(ggiraph)\n",
    "dt <- data.table(df_labelled)\n",
    "setnames(dt, c(\"Cluster\", \"V1\", \"V2\"), c(\"label\", \"x\", \"y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "p <- ggplot() +\n",
    "    geom_point(data=dt, aes(x=x, y=y, colour=factor(label)), alpha=0.5) +\n",
    "    scale_colour_discrete(\n",
    "        name=\"Cluster:\"\n",
    "    )\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.) K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "labels, cent = KMeans_Clustering(k, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cent_df = centroids_to_df(cent)\n",
    "df_labelled = pd.concat([pd.Series(labels), df], axis=1)\n",
    "df_labelled.columns = [\"Cluster\", \"x\", \"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i cent_df -i df_labelled\n",
    "library(ggplot2)\n",
    "library(gganimate)\n",
    "library(data.table)\n",
    "library(ggiraph)\n",
    "cent_dt <- data.table(cent_df)\n",
    "setnames(cent_dt, c(\"0\",\"1\", \"iter\"), c(\"x\", \"y\", \"iteration\"))\n",
    "dt <- data.table(df_labelled)\n",
    "setnames(dt, c(\"Cluster\"), c(\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "p <- ggplot() +\n",
    "    geom_point(data=dt, aes(x=x, y=y, colour=factor(label)), alpha=0.5) +\n",
    "    geom_point_interactive(\n",
    "        data=cent_dt[iteration==max(iteration)], \n",
    "        aes(x=x,y=y,fill=factor(k),tooltip=factor(k)), \n",
    "        colour=\"black\",\n",
    "        size=3, \n",
    "        shape=24\n",
    "    ) +\n",
    "    scale_fill_discrete(guide=F) +\n",
    "    scale_colour_discrete(\n",
    "        name=\"Cluster:\"\n",
    "    )\n",
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
