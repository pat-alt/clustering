{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization project: clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tic = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b8d0912b4457>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rc_params_in_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib_fname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_suppress_matplotlib_deprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m     \u001b[0mrcParamsOrig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRcParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m     \u001b[0;31m# This also checks that all rcParams are indeed listed in the template.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[0;31m# Assiging to rcsetup.defaultParams is left only for backcompat.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;31m# validate values on the way in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/_collections_abc.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, other, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keys\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, val)\u001b[0m\n\u001b[1;32m    675\u001b[0m                         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m                 \u001b[0mcval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key {key}: {ve}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/matplotlib/rcsetup.py\u001b[0m in \u001b[0;36mvalidate_color\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_color_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mis_color_like\u001b[0;34m(c)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;34m\"\"\"Return whether *c* can be interpreted as an RGB(A) color.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Special-case nth color syntax because it cannot be parsed during setup.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_is_nth_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m_is_nth_color\u001b[0;34m(c)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_nth_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;34m\"\"\"Return whether *c* can be interpreted as an item in the color cycle.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\AC[0-9]+\\Z\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/re.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \"\"\"Try to apply the pattern at the start of the string, returning\n\u001b[1;32m    188\u001b[0m     a Match object, or None if no match was found.\"\"\"\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfullmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/re.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# internal: compile pattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRegexFlag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mflags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "import functools\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.neighbors import DistanceMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum Spanning Tree Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing all the distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist(df):\n",
    "    size = len(df)\n",
    "    result = []\n",
    "    arr = df.values\n",
    "    for i in range(size):\n",
    "        for j in range(i+1,size):\n",
    "            # We use the package \"distance\" to compute the euclidean distance between\n",
    "            # the different points in the dataset\n",
    "            result.append([i, j, math.sqrt(sum((arr[i] - arr[j])**2))])\n",
    "\n",
    "    ordered_result = sorted(result, key=lambda t: t[::-1])\n",
    "    return ordered_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the minimum spanning tree it makes sense to differentiate the following cases:\n",
    "\n",
    "    case 1: None of them are in a cluster:\n",
    "    case 2: Only one of them is already in a cluster:\n",
    "    case 3: Both of them are already in a cluster:\n",
    "        --> 3a: Both of them are in the same cluster: do nothing\n",
    "        --> 3b: They are in different clusters: merge the 2 corresponding clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthetic = pd.read_csv('synthetic_clean.csv')\n",
    "N = len(df_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time0 = time.time()\n",
    "dist = DistanceMetric.get_metric('euclidean')\n",
    "df = df_synthetic[['V1', 'V2']].to_numpy()\n",
    "distance = dist.pairwise(df) \n",
    "sorted_edges = np.transpose(np.unravel_index(np.argsort(distance, axis=None), distance.shape)).tolist()\n",
    "sorted_edges= sorted_edges[5000::2] \n",
    "time1 = time.time()\n",
    "print(time1-time0)\n",
    "sorted_dist = np.sort(distance, axis=None).tolist()\n",
    "sorted_dist = sorted_dist[5000::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist(df):\n",
    "    N = len(df)\n",
    "    time0 = time.time()\n",
    "    dist = DistanceMetric.get_metric('euclidean')\n",
    "    df = df_synthetic[['V1', 'V2']].to_numpy()\n",
    "    distance = dist.pairwise(df)\n",
    "    sorted_edges = np.transpose(np.unravel_index(np.argsort(distance, axis=None), distance.shape)).tolist()\n",
    "    sorted_edges= sorted_edges[N::2]\n",
    "    time1 = time.time()\n",
    "    print(time1-time0)\n",
    "    return sorted_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_result = compute_dist(df_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mst, df_track = MST_clustering(4500, N, ordered_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MST_clustering(K, N, ordered_result):\n",
    "    # We initialize the variables that we will need in the outter for loop\n",
    "    cluster_dict = {}\n",
    "    k = 0\n",
    "    keep_track_df = pd.DataFrame({\"vertex\":range(N), \"in_tree\":0, \"cluster\":-1})\n",
    "    counter = 0 # number of edges\n",
    "    vertex = []\n",
    "    cluster_counter = 0 # number of unique clusters\n",
    "    i = 0\n",
    "    final_number_edges = N-K # final number of edges\n",
    "    for ite in ordered_result:\n",
    "        \n",
    "        # Here we set the condition to include as clusters the points that are left alone in the algorithm. \n",
    "        # At this point we will iterate through the rest of edges and we will add the points which have not been\n",
    "        # visited before (if they have been visited, they already are in one component of the MST)\n",
    "        \n",
    "        if counter == final_number_edges:\n",
    "            #print(\"Unique cluster before final \" + str(keep_track_df[keep_track_df.cluster!=-1].cluster.nunique()))\n",
    "            n_clusters_to_assign = len(keep_track_df[keep_track_df.cluster==-1])\n",
    "            keep_track_df.loc[keep_track_df.cluster==-1, \"cluster\"] = range(n_clusters_to_assign)\n",
    "            #print(\"clusters to fill: \" + str(n_clusters_to_assign))\n",
    "            for edge in range(N):\n",
    "                if edge not in vertex:\n",
    "                    k += 1\n",
    "                    cluster_dict[k] = [edge]\n",
    "                    vertex.append(edge)\n",
    "            #print(i)\n",
    "            return cluster_dict, keep_track_df\n",
    "        \n",
    "        # Here we want to know whether the vertices of the edge of this iteartion have already been included in any\n",
    "        # of the components of the MST. We assign a key to these two vertices. We assign key = -1 to the vertices \n",
    "        # that have not been visited before, and if the vertex have already been assigned to a cluster then we assign\n",
    "        # to him its component/cluster, so key = cluster.\n",
    "        \n",
    "        key_0 = -1\n",
    "        key_1 = -1\n",
    "        \n",
    "        if keep_track_df.iloc[ite[0]][2]==-1:\n",
    "            pass\n",
    "        else:\n",
    "            key_0 = keep_track_df.iloc[ite[0]][2]\n",
    "            \n",
    "        if keep_track_df.iloc[ite[1]][2]==-1:\n",
    "            pass\n",
    "        else:\n",
    "            key_1 = keep_track_df.iloc[ite[1]][2]\n",
    "            \n",
    "        # Now, we have four different cases. (a) None of the vertices have been added to a cluster, (b) one of the\n",
    "        # vertices has been added to a cluster, (c) both of them have been added to the same cluster (so we will not\n",
    "        # do anything given that we would be creating a cycle) and (d) both of them have been added to a cluster, but\n",
    "        # each of them is in a different cluster.\n",
    "        \n",
    "        # case (a)\n",
    "        \n",
    "        if (key_0 == -1) and (key_1 == -1):\n",
    "            k += 1\n",
    "            cluster_dict[k] = [ite[0]] \n",
    "            cluster_dict[k] += [ite[1]]\n",
    "            keep_track_df.iloc[ite[0]][2] = k\n",
    "            keep_track_df.iloc[ite[1]][2] = k\n",
    "            counter +=1\n",
    "            cluster_counter +=1\n",
    "            vertex.append(ite[0])\n",
    "            vertex.append(ite[1])\n",
    "            \n",
    "        # case (b)\n",
    "        \n",
    "        elif (key_0 == -1) and (key_1 != -1):\n",
    "            cluster_dict[key_1] += [ite[0]]\n",
    "            assign_to_0 = keep_track_df[keep_track_df.vertex == ite[1]].cluster.values[0]\n",
    "            keep_track_df.loc[keep_track_df.vertex == ite[0], \"cluster\"] = assign_to_0\n",
    "            counter +=1\n",
    "            vertex.append(ite[0])\n",
    "        elif (key_0 != -1) and (key_1 == -1):\n",
    "            cluster_dict[key_0] += [ite[1]]\n",
    "            assign_to_1 = keep_track_df[keep_track_df.vertex == ite[0]].cluster.values[0]\n",
    "            keep_track_df.loc[keep_track_df.vertex == ite[1], \"cluster\"] = assign_to_1\n",
    "            counter +=1\n",
    "            vertex.append(ite[1])\n",
    "            \n",
    "        # case (c) and (d)\n",
    "        else:\n",
    "            \n",
    "            # case (c)\n",
    "            if key_0 == key_1:\n",
    "                pass\n",
    "            \n",
    "            # case (d)\n",
    "            else:\n",
    "                cluster_dict[key_0] += cluster_dict[key_1]\n",
    "                del cluster_dict[key_1]\n",
    "                assign_to_1 = keep_track_df[keep_track_df.vertex == ite[0]].cluster.values[0]\n",
    "                cluster_to_delete = keep_track_df[keep_track_df.vertex == ite[1]].cluster.values[0]\n",
    "                keep_track_df.loc[keep_track_df.cluster == cluster_to_delete, \"cluster\"] = assign_to_1\n",
    "                cluster_counter -=1\n",
    "                counter +=1\n",
    "        i +=1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(cluster_dict):\n",
    "    ind_list = []\n",
    "    clust_list = []\n",
    "\n",
    "    for k, v in cluster_dict.items():\n",
    "        [clust_list.append(k) for i in v]\n",
    "        [ind_list.append(val) for val in v]\n",
    "\n",
    "    df_s = pd.DataFrame()\n",
    "    df_s['index'] = ind_list\n",
    "    df_s['Cluster'] = clust_list\n",
    "    \n",
    "    df_s = df_s.sort_values('index').set_index('index')\n",
    "    return df_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Davies Bouldin algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Davies_Bouldin(cluster_dict, df):\n",
    "    \n",
    "    # We compute the index of the clusters \n",
    "    \n",
    "    clusters = []\n",
    "    for n in cluster_dict:\n",
    "        clusters.append(n)\n",
    "        \n",
    "    # We compute the coordinates of the centroid for each cluster\n",
    "    \n",
    "    centroid_dict = {}\n",
    "    for n in clusters:\n",
    "        mask = (df.Cluster == n)\n",
    "        cluster_df = df[mask]\n",
    "        centroid = cluster_df.mean()\n",
    "        centroid = centroid.to_list()\n",
    "        list_ = []\n",
    "        for el in range(0,len(list(df.columns[:-1]))):\n",
    "            list_.append(centroid[el])\n",
    "            centroid_dict[n] = list_\n",
    "            \n",
    "    # We compute the measure of the scatter within the cluster, S_i\n",
    "    \n",
    "    S_i = {}\n",
    "    for n in cluster_dict:\n",
    "        sum_ = 0\n",
    "        for el in cluster_dict[n]:\n",
    "            sum_ += distance.euclidean(df.iloc[el][:-1],centroid_dict[n])\n",
    "            average = sum_/len(cluster_dict[n])\n",
    "            S_i[n] = average\n",
    "            \n",
    "    # Compute the distance between the clusters, i.e., the distance between the centroids of the clusters\n",
    "    \n",
    "    M_ij = {}\n",
    "    centroids = list(centroid_dict.keys())\n",
    "    for i in range(0,len(centroids)):\n",
    "        for j in range(i+1,len(centroids)):\n",
    "            d = distance.euclidean(centroid_dict[centroids[i]],centroid_dict[centroids[j]])\n",
    "            M_ij[(centroids[i],centroids[j])] = d\n",
    "    \n",
    "    # We compute D_i by computig in a forloop R_ij\n",
    "    \n",
    "    dispersion = list(S_i.keys())\n",
    "    D_i = {}\n",
    "    for i in range(0, len(dispersion)):\n",
    "        D_i[dispersion[i]] = 0\n",
    "        for j in range(0,len(dispersion)):\n",
    "            if i!=j:\n",
    "                try:\n",
    "                    R_ij = (S_i[dispersion[i]]+S_i[dispersion[j]])/(M_ij[(dispersion[i],dispersion[j])])\n",
    "                    if R_ij >= D_i[dispersion[i]]:\n",
    "                        D_i[dispersion[i]] = R_ij\n",
    "                except:\n",
    "                    R_ij = (S_i[dispersion[i]]+S_i[dispersion[j]])/(M_ij[(dispersion[j],dispersion[i])])\n",
    "                    if R_ij >= D_i[dispersion[i]]:\n",
    "                        D_i[dispersion[i]] = R_ij\n",
    "                        \n",
    "    count = 0\n",
    "    for n in D_i:\n",
    "        count += D_i[n]\n",
    "    DB = count/len(D_i)\n",
    "    return DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dunn Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_within(ordered_result, cluster_dict):\n",
    "    for i in range(len(ordered_result)):\n",
    "        for k,v in cluster_dict.items():\n",
    "            if ordered_result[-i-1][0] in v and ordered_result[-i-1][1] in v:\n",
    "                return ordered_result[-i-1][2]\n",
    "\n",
    "def min_between(ordered_result, cluster_dict):\n",
    "    for i in range(len(ordered_result)):\n",
    "        for k,v in cluster_dict.items():\n",
    "            if ordered_result[i][0] in v and ordered_result[i][1] not in v:\n",
    "                return ordered_result[i][2]\n",
    "\n",
    "def dunn(ordered_result, cluster_dict):\n",
    "    num = max_within(ordered_result, cluster_dict)\n",
    "    den = min_between(ordered_result, cluster_dict)\n",
    "    return num/den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the dataset with 2 columns ('Synthetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_synthetic = pd.read_csv('synthetic_clean.csv')\n",
    "N = len(df_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ordered_result = pd.read_csv('ordered_result_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ordered_result = ordered_result.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#ordered_result = compute_dist(df_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordered_result_csv = pd.DataFrame(ordered_result)\n",
    "#ordered_result_csv.to_csv('ordered_result_csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_MST(K, df_synthetic):\n",
    "    #tic1 = time.time()\n",
    "    cluster_dict = MST_clustering(K, N, ordered_result)\n",
    "    #tic2 = time.time()\n",
    "    #print(tic2 - tic1)\n",
    "    df_synthetic['Cluster']= get_df(cluster_dict)['Cluster']\n",
    "    #tic3 = time.time()    \n",
    "    #print(tic3 - tic2)\n",
    "    DB_MST = Davies_Bouldin(cluster_dict,df_synthetic)\n",
    "    #tic4 = time.time()\n",
    "    #print(tic4 - tic3)\n",
    "    Dunn_MST = dunn(ordered_result, cluster_dict)\n",
    "    #tic5 = time.time()    \n",
    "    #print(tic5 - tic4)\n",
    "    #package_MST = davies_bouldin_score(df.drop('Cluster',axis=1), labels = df['Cluster'].to_list())\n",
    "    \n",
    "    return DB_MST, Dunn_MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ordered_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "DB, Dunn = main_MST(15, df_synthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the dataset with 5 columns ('Thyroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thyroid = pd.read_csv('thyroid_clean.csv')\n",
    "N = len(df_thyroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ordered_result = compute_dist(df_thyroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_clusters = []\n",
    "DB_MST = []\n",
    "Dunn_MST = []\n",
    "for K in range(2, N+1, 10):\n",
    "    n_clusters.append(K)\n",
    "    DB, Dunn = main_MST(K, df_thyroid)\n",
    "    DB_MST.append(DB)\n",
    "    Dunn_MST.append(Dunn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot(n_clusters,DB_MST)\n",
    "plt.show()\n",
    "sns.lineplot(n_clusters,Dunn_MST)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly assign indeces of cluster centroids:\n",
    "def initiate_centroids(k, df):\n",
    "    centroids = []\n",
    "    random.seed(10) #for reproducability  \n",
    "    #generate random centroid indeces\n",
    "    initial_index_centroid = random.sample(range(0, len(df)), k)\n",
    "    #find the data points corresponding the the indeces:\n",
    "    for i in initial_index_centroid:\n",
    "        centroids.append(df.loc[i])\n",
    "    return np.array(centroids)\n",
    "\n",
    "# To find the closest centroid to each data point:\n",
    "def findClosestCentroids(centroids, df):\n",
    "    assigned_centroid = []\n",
    "    arr = df.to_numpy()\n",
    "    #iterate over every data point in the dataframe:\n",
    "    for row in arr:\n",
    "        distance=[]\n",
    "        #find distance of data point with each cluster:\n",
    "        for center in centroids:          \n",
    "            distance.append(np.linalg.norm(row-center))     \n",
    "        #assign data point to closest cluster:\n",
    "        assigned_centroid.append(np.argmin(distance))\n",
    "    return assigned_centroid\n",
    "\n",
    "\n",
    "#To update the centroid of the clusters:\n",
    "def calc_centroids(clusters, df): \n",
    "    #initiate empty list for new centroids of each cluster:\n",
    "    new_centroids = []\n",
    "    #df including each point and its respective cluster \n",
    "    arr = np.c_[df.to_numpy(), clusters]\n",
    "    #iterate over the distinct clusters\n",
    "    for c in np.unique(clusters):    \n",
    "        #take out the data points corresponding to each cluster:\n",
    "        current_arr = arr[arr[:, -1] == c][:,:-1]   \n",
    "        #find the new cluster centroid which is the mean of the clusters we already assigned\n",
    "        cluster_mean = current_arr.mean(axis=0)                                          \n",
    "        #append the new centroid\n",
    "        new_centroids.append(cluster_mean)   \n",
    "    return np.vstack(new_centroids)\n",
    "\n",
    "#Recursively find and update cluster centroids:\n",
    "#n: number of clusters, df: dataframe of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def KMeans_Clustering(k, df):\n",
    "    #initiate centroids:\n",
    "    old_centroids = initiate_centroids(k, df)\n",
    "    #initiate new centroids (do this for first while loop condition to work)\n",
    "    new_centroids = calc_centroids(findClosestCentroids(old_centroids, df), df)\n",
    "    # Iterate until the absoulte difference between the coordinates of all centroids \n",
    "    #does not change, ie: the k-means algorithm converges:\n",
    "    centroids = [old_centroids, new_centroids]\n",
    "    i = 0\n",
    "    while np.any(np.absolute(np.array(new_centroids) - np.array(old_centroids)) > 0):\n",
    "        #set new centroids as the old ones:\n",
    "        i += 1\n",
    "        print(i)\n",
    "        old_centroids = new_centroids\n",
    "        #find the new ones:\n",
    "        new_centroids = calc_centroids(findClosestCentroids(old_centroids, df), df)\n",
    "        centroids += [new_centroids]\n",
    "        #print(np.array(new_centroids))\n",
    "    return findClosestCentroids(new_centroids, df), centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means for sysnthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('synthetic_clean.csv')\n",
    "N = len(df)\n",
    "k = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "labels, cent = KMeans_Clustering(k, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroids_to_df(cent):\n",
    "    df = pd.DataFrame(cent[0])\n",
    "    k = np.shape(cent)[1]\n",
    "    df['k'] = range(k)\n",
    "    df['iter'] = 0\n",
    "    for i in range(1,np.shape(cent)[0]):\n",
    "        df_new = pd.DataFrame(cent[i])\n",
    "        df_new['k'] = range(k)\n",
    "        df_new['iter'] = i \n",
    "        df = pd.concat([df, df_new], axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cent_df = centroids_to_df(cent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_labelled = pd.concat([pd.Series(labels), df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i cent_df -i df_labelled\n",
    "library(ggplot2)\n",
    "library(gganimate)\n",
    "library(data.table)\n",
    "library(ggiraph)\n",
    "cent_dt <- data.table(cent_df)\n",
    "setnames(cent_dt, c(\"0\",\"1\", \"iter\"), c(\"x\", \"y\", \"iteration\"))\n",
    "dt <- data.table(df_labelled)\n",
    "setnames(dt, c(\"0\",\"V1\",\"V2\"), c(\"label\",\"x\", \"y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "p <- ggplot() +\n",
    "    geom_point(data=dt, aes(x=x, y=y, colour=factor(label)), alpha=0.5) +\n",
    "    geom_point_interactive(\n",
    "        data=cent_dt[iteration==max(iteration)], \n",
    "        aes(x=x,y=y,fill=factor(k),tooltip=factor(k)), \n",
    "        colour=\"black\",\n",
    "        size=3, \n",
    "        shape=24\n",
    "    ) +\n",
    "    scale_fill_discrete(guide=F) +\n",
    "    scale_colour_discrete(\n",
    "        name=\"Cluster:\"\n",
    "    )\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_dict_2(df):\n",
    "    cluster_dict = {}\n",
    "    visited = []\n",
    "    for n in range(0,len(df)):\n",
    "        if int(df.iloc[n][2]) in visited:\n",
    "            cluster_dict[int(df.iloc[n][2])] += [n]\n",
    "        else:\n",
    "            cluster_dict[int(df.iloc[n][2])] = [n]\n",
    "            visited.append(int(df.iloc[n][2]))\n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_dict_5(df):\n",
    "    cluster_dict = {}\n",
    "    visited = []\n",
    "    for n in range(0,len(df)):\n",
    "        if int(df.iloc[n][5]) in visited:\n",
    "            cluster_dict[int(df.iloc[n][5])] += [n]\n",
    "        else:\n",
    "            cluster_dict[int(df.iloc[n][5])] = [n]\n",
    "            visited.append(int(df.iloc[n][5]))\n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the dataset with 2 columns ('Synthetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N = 200\n",
    "#X1, X2 = simulate_data_2(N)\n",
    "#df = pd.DataFrame({'X1': X1, 'X2': X2}, columns=['X1', 'X2'])\n",
    "df = pd.read_csv('synthetic_clean.csv')\n",
    "\n",
    "def main_KMeans(k, df):\n",
    "    \n",
    "    df['Cluster'] = KMeans_Clustering(k, df_synthetic)\n",
    "    cluster_dict = get_cluster_dict_2(df_synthetic)\n",
    "    DB_KMeans = Davies_Bouldin(cluster_dict, df_synthetic)\n",
    "    Dunn_KMeans = dunn(ordered_result, cluster_dict)\n",
    "    \n",
    "    return DB_KMeans, Dunn_KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_KMeans, Dunn_KMeans = main_KMeans(15, df_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dunn_KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the dataset with 5 columns ('Thyroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thyroid = pd.read_csv('thyroid_clean.csv')\n",
    "N = len(df_thyroid)\n",
    "ordered_result = compute_dist(df_thyroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_KMeans(k, df):\n",
    "    \n",
    "    df['Cluster'] = KMeans_Clustering(k, df_thyroid)\n",
    "    cluster_dict = get_cluster_dict_5(df_thyroid)\n",
    "    DB_KMeans = Davies_Bouldin(cluster_dict, df_thyroid)\n",
    "    Dunn_KMeans = dunn(ordered_result, cluster_dict)\n",
    "    #package_MST = davies_bouldin_score(df.drop('Cluster',axis=1), labels = df['Cluster'].to_list())\n",
    "\n",
    "    \n",
    "    return DB_KMeans, Dunn_KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_clusters = []\n",
    "DB_KMeans = []\n",
    "Dunn_KMeans = []\n",
    "for K in range(2, N+1, 10):\n",
    "    n_clusters.append(K)\n",
    "    DB, Dunn = main_KMeans(K, df_thyroid)\n",
    "    DB_KMeans.append(DB)\n",
    "    Dunn_KMeans.append(Dunn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot(n_clusters, DB_KMeans)\n",
    "plt.show()\n",
    "sns.lineplot(n_clusters, Dunn_KMeans)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = time.time()\n",
    "toc-tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(toc-tic)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
